import 'dart:io';
import 'package:http/http.dart' as http;
import 'package:logger/logger.dart';
import 'package:record/record.dart';
import 'package:path_provider/path_provider.dart';
import 'dart:convert';

/// Servicio de transcripci√≥n usando OpenAI Whisper API (Speech-to-Text)
/// - Mejor precisi√≥n y comprensi√≥n del espa√±ol
/// - Manejo inteligente del audio sin interrupciones
/// - Gesti√≥n eficiente de contexto de actividad
class OpenAiSttService {
  static final OpenAiSttService _instance = OpenAiSttService._internal();
  factory OpenAiSttService() => _instance;
  OpenAiSttService._internal();

  final _logger = Logger();
  final _audioRecorder = AudioRecorder();

  // Configuraci√≥n de OpenAI
  // IMPORTANTE: Cargar desde variables de entorno, NO hardcodear
  static const String _openAiApiKey = 'YOUR_OPENAI_API_KEY_HERE';
  static const String _openAiEndpoint =
      'https://api.openai.com/v1/audio/transcriptions';

  bool _isRecording = false;
  String? _currentRecordingPath;
  DateTime? _recordingStartTime;
  bool _shouldStopRecording = false;

  // Control de contexto para no repetir palabras clave
  String _lastActivityContext = '';
  List<String> _previousTranscriptions = [];

  bool get isRecording => _isRecording;

  /// Inicializar el servicio de grabaci√≥n de audio
  Future<void> initialize() async {
    try {
      // Verificar permisos de micr√≥fono
      final hasPermission = await _audioRecorder.hasPermission();
      if (!hasPermission) {
        _logger.w('Audio recording permission denied');
        return;
      }
      _logger.i('‚úì OpenAI STT service initialized');
    } catch (e) {
      _logger.e('Error initializing STT service: $e');
    }
  }

  /// Iniciar grabaci√≥n de audio para actividad espec√≠fica
  /// [activityContext]: Contexto de la actividad (ej: "dictado", "memoria", "velocidad")
  /// [maxDuration]: Duraci√≥n m√°xima en segundos
  Future<bool> startRecording({
    String activityContext = 'actividad',
    int maxDuration = 30,
  }) async {
    if (_isRecording) {
      _logger.w('Already recording');
      return false;
    }

    try {
      _lastActivityContext = activityContext;
      _shouldStopRecording = false;
      _recordingStartTime = DateTime.now();

      // Obtener ruta temporal para guardar audio
      final dir = await getTemporaryDirectory();
      _currentRecordingPath =
          '${dir.path}/stt_recording_${DateTime.now().millisecondsSinceEpoch}.wav';

      // Iniciar grabaci√≥n con configuraci√≥n √≥ptima
      await _audioRecorder.start(
        RecordConfig(
          encoder: AudioEncoder.wav,
          bitRate: 128000, // 128 kbps para buena calidad
          sampleRate: 16000, // 16 kHz ideal para Whisper
          numChannels: 1, // Mono es suficiente
          autoGain: true, // Ajuste autom√°tico de ganancia
          echoCancel: true, // Cancelaci√≥n de eco
        ),
        path: _currentRecordingPath!,
      );

      _isRecording = true;
      _logger.i(
        'üé§ Recording started for: $activityContext (max ${maxDuration}s)',
      );

      // Auto-stop despu√©s de duraci√≥n m√°xima
      _scheduleAutoStop(maxDuration);

      return true;
    } catch (e) {
      _logger.e('Error starting recording: $e');
      _isRecording = false;
      return false;
    }
  }

  /// Agendar detenci√≥n autom√°tica de grabaci√≥n
  void _scheduleAutoStop(int maxDuration) {
    Future.delayed(Duration(seconds: maxDuration), () {
      if (_isRecording && !_shouldStopRecording) {
        _logger.i('Auto-stopping recording after $maxDuration seconds');
        _shouldStopRecording = true;
      }
    });
  }

  /// Detener grabaci√≥n y obtener transcripci√≥n de OpenAI
  /// Retorna el texto transcrito (sin anuncios de finalizaci√≥n)
  Future<String?> stopRecordingAndTranscribe() async {
    if (!_isRecording) {
      _logger.w('Not recording');
      return null;
    }

    try {
      _isRecording = false;

      // Detener grabaci√≥n
      final recordingPath = await _audioRecorder.stop();
      _logger.i('üõë Recording stopped: $recordingPath');

      if (recordingPath == null || recordingPath.isEmpty) {
        _logger.e('No recording path returned');
        return null;
      }

      // Verificar que el archivo existe y tiene contenido
      final file = File(recordingPath);
      if (!await file.exists()) {
        _logger.e('Recording file does not exist: $recordingPath');
        return null;
      }

      final fileSize = await file.length();
      if (fileSize < 1000) {
        // Menos de 1KB probablemente es ruido/silencio
        _logger.w('Recording too short or empty: ${fileSize}B');
        return null;
      }

      _logger.i(
        'üì§ Sending audio to OpenAI Whisper API (${(fileSize / 1024).toStringAsFixed(1)}KB)...',
      );

      // Enviar a OpenAI Whisper
      final transcription = await _transcribeWithOpenAi(recordingPath);

      // Limpiar archivo temporal
      await file
          .delete()
          .then((_) {
            _logger.i('Temp audio file deleted');
          })
          .catchError((e) {
            _logger.w('Could not delete temp file: $e');
          });

      return transcription;
    } catch (e) {
      _logger.e('Error stopping recording: $e');
      _isRecording = false;
      return null;
    }
  }

  /// Transcribir audio usando OpenAI Whisper API
  /// Retorna solo el texto transcrito, sin mensajes adicionales
  Future<String?> _transcribeWithOpenAi(String audioPath) async {
    try {
      // Crear request multipart para la API de OpenAI
      final request = http.MultipartRequest('POST', Uri.parse(_openAiEndpoint));

      // Agregar headers
      request.headers['Authorization'] = 'Bearer $_openAiApiKey';

      // Agregar archivo de audio
      request.files.add(
        await http.MultipartFile.fromPath(
          'file',
          audioPath,
          filename: 'audio.wav',
        ),
      );

      // Agregar par√°metros
      request.fields['model'] = 'whisper-1';
      request.fields['language'] = 'es'; // Espa√±ol expl√≠cito
      request.fields['response_format'] = 'json';

      // Opcional: Agregar prompt para mejorar precisi√≥n
      // en contextos espec√≠ficos
      if (_lastActivityContext.isNotEmpty) {
        String prompt = _generateContextPrompt(_lastActivityContext);
        request.fields['prompt'] = prompt;
      }

      // Enviar request
      _logger.i('üöÄ Sending to OpenAI Whisper API...');
      final response = await request.send().timeout(
        const Duration(seconds: 60),
        onTimeout: () {
          throw Exception('Whisper API timeout after 60 seconds');
        },
      );

      if (response.statusCode != 200) {
        _logger.e('OpenAI API error: ${response.statusCode}');
        final body = await response.stream.bytesToString();
        _logger.e('Response: $body');
        return null;
      }

      // Parsear respuesta
      final responseBody = await response.stream.bytesToString();
      final jsonResponse = jsonDecode(responseBody);

      final transcribedText = (jsonResponse['text'] as String?)?.trim() ?? '';

      if (transcribedText.isEmpty) {
        _logger.w('Empty transcription received');
        return null;
      }

      _logger.i('‚úì Transcribed: "$transcribedText"');

      // Procesar el texto para mejorar precisi√≥n
      final processedText = _postProcessTranscription(transcribedText);

      // Guardar en hist√≥rico para contexto futuro
      _previousTranscriptions.add(processedText);
      if (_previousTranscriptions.length > 10) {
        _previousTranscriptions.removeAt(0);
      }

      return processedText;
    } on Exception catch (e) {
      _logger.e('Exception: $e');
      return null;
    } catch (e) {
      _logger.e('Error transcribing with OpenAI: $e');
      return null;
    }
  }

  /// Generar prompt contextual para mejorar precisi√≥n de Whisper
  String _generateContextPrompt(String activityContext) {
    // Vocabulario espec√≠fico seg√∫n el tipo de actividad
    final vocabularyMap = {
      'dictado': 'Palabras espa√±olas comunes, enfocarse en pronunciaci√≥n clara',
      'memoria': 'Secuencias de letras, n√∫meros de una en una, sin palabras',
      'velocidad': 'Texto literario corto con palabras comunes en espa√±ol',
      'audici√≥n': 'Letras individuales pronunciadas claramente',
      'ortograf√≠a': 'Palabras espa√±olas con enfoque en ortograf√≠a',
      'discriminaci√≥n': 'Pares de palabras similares que suenan parecido',
    };

    return vocabularyMap[activityContext] ??
        'Transcribir texto hablado en espa√±ol de manera precisa y clara';
  }

  /// Post-procesar transcripci√≥n para mejorar precisi√≥n
  /// - Normalizar puntuaci√≥n
  /// - Eliminar artefactos
  /// - Mantener contexto de actividad
  String _postProcessTranscription(String text) {
    var processed = text.trim();

    // 1. Normalizar espacios m√∫ltiples
    processed = processed.replaceAll(RegExp(r'\s+'), ' ');

    // 2. Remover puntuaci√≥n innecesaria al final (excepto en dictado)
    if (_lastActivityContext != 'dictado') {
      processed = processed.replaceAll(RegExp(r'[.,!?]+$'), '');
    }

    // 3. Normalizar may√∫sculas seg√∫n contexto
    if (_lastActivityContext == 'memoria' ||
        _lastActivityContext == 'audici√≥n') {
      // Para secuencias de letras, mantener may√∫sculas
      processed = processed.toUpperCase();
    } else if (_lastActivityContext == 'dictado') {
      // Para dictado, primera letra may√∫scula
      if (processed.isNotEmpty) {
        processed = processed[0].toUpperCase() + processed.substring(1);
      }
    }

    // 4. Eliminar palabras de relleno comunes
    final fillerWords = ['este', 'uh', 'um', 'ya', 'bueno'];
    for (final filler in fillerWords) {
      final regex = RegExp(r'\b' + filler + r'\b', caseSensitive: false);
      processed = processed
          .replaceAll(regex, '')
          .replaceAll(RegExp(r'\s+'), ' ');
    }

    // 5. Corregir errores comunes de Whisper en espa√±ol
    final corrections = {
      r'\b√©sta\b': 'esta',
      r'\b√©ste\b': 'este',
      r'\b√©so\b': 'eso',
      r'\besl√°bila\b': 's√≠laba',
    };

    corrections.forEach((pattern, replacement) {
      processed = processed.replaceAll(
        RegExp(pattern, caseSensitive: false),
        replacement,
      );
    });

    return processed.trim();
  }

  /// Parar grabaci√≥n sin transcribir (√∫til si el usuario cancela)
  Future<void> cancelRecording() async {
    if (!_isRecording) return;

    try {
      _isRecording = false;
      _shouldStopRecording = true;

      final recordingPath = await _audioRecorder.stop();

      // Limpiar archivo
      if (recordingPath != null) {
        final file = File(recordingPath);
        await file
            .delete()
            .then((_) {
              _logger.i('Cancelled recording deleted');
            })
            .catchError((e) {
              _logger.w('Could not delete cancelled recording: $e');
              return null;
            });
      }

      _logger.i('üö´ Recording cancelled');
    } catch (e) {
      _logger.e('Error cancelling recording: $e');
    }
  }

  /// Obtener duraci√≥n de grabaci√≥n actual (en segundos)
  int getCurrentRecordingDuration() {
    if (_recordingStartTime == null) return 0;
    return DateTime.now().difference(_recordingStartTime!).inSeconds;
  }

  /// Limpiar recursos
  void dispose() {
    _audioRecorder.dispose();
    _previousTranscriptions.clear();
  }
}
